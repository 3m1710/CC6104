%\documentclass[mathserif]{beamer}
\documentclass[handout]{beamer}
%\usetheme{Goettingen}
\usetheme{Warsaw}
%\usetheme{Singapore}
%\usetheme{Frankfurt}
%\usetheme{Copenhagen}
%\usetheme{Szeged}
%\usetheme{Montpellier}
%\usetheme{CambridgeUS}
%\usecolortheme{}
%\setbeamercovered{transparent}
\usepackage[english, activeacute]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{dsfont}
\usepackage{graphics}
\usepackage{cases}
\usepackage{graphicx}
\usepackage{pgf}
\usepackage{epsfig}
\usepackage{amssymb}
\usepackage{multirow}	
\usepackage{amstext}
\usepackage[ruled,vlined,lined]{algorithm2e}
\usepackage{amsmath}
\usepackage{epic}
\usepackage{epsfig}
\usepackage{fontenc}
\usepackage{framed,color}
\usepackage{palatino, url, multicol}
\usepackage{listings}
%\algsetup{indent=2em}
\newcommand{\factorial}{\ensuremath{\mbox{\sc Factorial}}}
\newcommand{\BIGOP}[1]{\mathop{\mathchoice%
{\raise-0.22em\hbox{\huge $#1$}}%
{\raise-0.05em\hbox{\Large $#1$}}{\hbox{\large $#1$}}{#1}}}
\newcommand{\bigtimes}{\BIGOP{\times}}
\vspace{-0.5cm}
\title{Deisgn of Experiments \& Hypothesis Testing}
\vspace{-0.5cm}
\author[Felipe Bravo Márquez]{\footnotesize
%\author{\footnotesize  
 \textcolor[rgb]{0.00,0.00,1.00}{Felipe José Bravo Márquez}} 
\date{ \today }






\begin{document}
\begin{frame}
\titlepage


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Useful references: http://www.buders.com/UNIVERSITE/Universite_Dersleri/istatistik/sampling_distributions_and_point_estimation_of_parameters.pdf
% http://homepage.divms.uiowa.edu/~rdecook/stat2020/notes/ch7_pt1.pdf



\begin{frame}{Design of Experiments}
\scriptsize{
\begin{itemize}
 \item sfsdf

\end{itemize}



} 
\end{frame}


\begin{frame}{Hypothesis Testing}
\scriptsize{
\begin{itemize}
 \item When we want to test whether some assumed \textbf{property} about a population is contrasted with a statistical sample we use a \textbf{hypothesis test}.
\item The test consists of the following hypotheses:
 \begin{itemize}
\scriptsize{
\item \textbf{Null Hypothesis} $H_{0}$: Symbolizes the current situation. What has been considered real up to the present.
\item \textbf{Alternative Hypothesis} $H_{a}$: it is the alternative model that we want to consider. 
}
 \end{itemize}
\item The idea is to find enough \textbf{statistical evidence} to reject $H_{0}$ and be able to conclude $H_{a}$.
\item If we do not get enough statistical evidence \textbf{we fail to reject} $H_{0}$

\end{itemize}



} 
\end{frame}



\begin{frame}{Hypothesis Testing (2)}
\scriptsize{

\begin{block}{Methodology to Perform a Hypothesis Test}
\begin{itemize}
 \item Choose a null hypothesis $H_0$ and alternative $H_a$.
 \item Set a test significance level $\alpha$.
 \item Calculate a statistic $T$ from the data.
 \item  The statistic $T$ is usually a standardized value that we can check in a distribution table.
 \item Define a rejection criterion for the null hypothesis. It is usually a critical value $c$.
\end{itemize}
\end{block}



} 
\end{frame}



\begin{frame}[fragile]{Hypothesis Testing (3)}
\scriptsize{
\begin{itemize}
 \item Example: It is known that the average number of hours of monthly Internet use in Chile is 30 hours.
 \item Suppose we want to show that the average is different from that value.
 \item We would have that $H_0: \mu=30$ and $H_{a}: \mu \neq 30$
 \item Let's set $\alpha=0.05$ and collect 100 observations.
 \item Suppose we get $\overline{X_{n}}=28$ and $s=10$
 \item  One way to test is to construct a confidence interval for $\mu$ and see if $H_{0}$ is in the interval.
\begin{verbatim}
> 28-qt(p=0.975,99)*10/sqrt(100)
[1] 26.01578
> 28+qt(p=0.975,99)*10/sqrt(100)
[1] 29.98422 
\end{verbatim}
\item The interval would be the acceptance zone of $H_0$ and anything outside of this would be my rejection region.
\item Since 30 is in the rejection region, I reject my null hypothesis with $5\%$ confidence.
\end{itemize}



} 
\end{frame}


\begin{frame}[fragile]{Hypothesis Testing (4)}
\scriptsize{
\begin{itemize}
 \item Another way to perform the test is to compute the statistic  $T=\frac{\overline{X_{n}}-\mu_{o}}{\frac{s}{\sqrt{n}}}$
 \item  In this case it would be \begin{displaymath}
                           T=\frac{28-30}{\frac{10}{\sqrt{100}}}=-2
                          \end{displaymath}
\item Since $H_{a}: \mu \neq 30$, we have a two-sided test, where the acceptance region is.
\begin{displaymath}
 t_{n-1,1-\alpha/2}<T<t_{n-1,\alpha/2}
\end{displaymath}
\begin{verbatim}
 > qt(0.025,99)
[1] -1.984217
> qt(0.975,99)
[1] 1.984217
\end{verbatim}
\item Since $T$ is in the rejection region, we reject the null hypothesis.

\end{itemize}



} 
\end{frame}


\begin{frame}[fragile]{Hypothesis Testing (5)}
\scriptsize{
\begin{itemize}
 \item Generally, in addition to knowing whether we reject or fail to reject a null hypothesis we want to quantify the evidence we have against it.
 \item A \textbf{p-value} is defined as the probability of obtaining an outcome at least as extreme as that observed in the data given that the null hypothesis is true.
 \item ``Extreme'' means far from the null hypothesis and favorable for the alternative hypothesis.
 \item If the \textbf{p-value} s less than the significance level $\alpha$, we reject $H_{0}$ 
 \item Example:
\begin{verbatim}
> data(iris)
> mu<-3 # null hypothesis
> alpha<-0.05
> n<-length(iris$Petal.Length)
> xbar<-mean(iris$Petal.Length)
> s<-sd(iris$Petal.Length)
> se<-s/sqrt(n)
> t<-(xbar-mu)/(s/sqrt(n))
> pvalue<-2*pt(-abs(t),df=n-1)
> pvalue
[1] 4.94568e-07 # is less than 0.05 then we reject H0
\end{verbatim}
\end{itemize}

 


}
\end{frame}

\begin{frame}[fragile]{Hypothesis Testing (6)}
\scriptsize{
\begin{itemize}
 \item The elegant way to do it in R:
\end{itemize}

\begin{verbatim}
> t.test(x=iris$Petal.Length,mu=3)

	One Sample t-test

data:  iris$Petal.Length 
t = 5.2589, df = 149, p-value = 4.946e-07
alternative hypothesis: true mean is not equal to 3 
95 percent confidence interval:
 3.473185 4.042815 
sample estimates:
mean of x 
    3.758 
\end{verbatim}
}



\end{frame}

\begin{frame}{Hypothesis Testing (7)}
 \scriptsize{

\begin{itemize}
 \item We have two types of errors when we perform a hypothesis test
 \item Type I error: it is when we reject the null hypothesis when it is true.
 \item This error is equivalent to the significance level $\alpha$. 
 \item Type II error: is when the null hypothesis is false but we do not have statistical evidence to reject it.
 \item To mitigate type I errors we generally use smaller values of $\alpha$.
 \item To mitigate type II errors we generally work with larger samples.
 \item There is a trade-off between type I and type II errors. 
\end{itemize}

 \begin{table}
\begin{tabular}{c | c c}
\hline
  & Retain $H_0$ &  Reject $H_{0}$   \\ 
\hline
$H_0$ true & \checkmark & type I \\
$H_1$ true & type II error & \checkmark \\
\hline
\end{tabular}
\end{table}

}
\end{frame}

\begin{frame}{Statistical Power}
 
\end{frame}

\begin{frame}{Critics to Hypothesis Testing}
 
\end{frame}


\begin{frame}{FOUR CARDINAL RULES OF STATISTICS by Daniela Witten}
\scriptsize{
\begin{itemize}
% source: https://twitter.com/daniela_witten/status/1312180955801505794
 \item ONE:  CORRELATION DOES NOT IMPLY CAUSATION.  Yes, I know you know this, but it’s so easy to forget! Yeah, YOU OVER THERE, you with the p-value of 0.0000001 — yes, YOU!! That’s not causation.
 \item No matter how small the p-value for a regression of IQ onto shoe size is, that doesn’t mean that big feet cause smarts!!  It just means that grown-ups tend to have bigger feet and higher IQs than kids.
 \item So, unless you can design your study to uncover causation (very hard to do in most practical settings — the field of causal inference is devoted to understanding the settings in which it is possible), the best you can do is to discover correlations.  Sad but true.
 \item TWO:  A P-VALUE IS JUST A TEST OF SAMPLE SIZE.  Read that again — I mean what I said!  If your null hypothesis doesn't hold (and null hypotheses never hold IRL) then the larger your sample size, the smaller your p-value will tend to be.
 \item If you’re testing whether mean=0 and actually the truth is that mean=0.000000001, and if you have a large enough sample size, then YOU WILL GET A TINY P-VALUE.
 \item Why does this matter? In many contemporary settings (think: the internet), sample sizes are so huge that we can get TINY p-values even when the deviation from the null hypothesis is negligible. In other words, we can have STATISTICAL significance w/o PRACTICAL significance.
 \end{itemize}

} 
\end{frame}

\begin{frame}{FOUR CARDINAL RULES OF STATISTICS by Daniela Witten}
\scriptsize{
\begin{itemize}
 \item Often, people focus on that tiny p-value, and the fact that the effect is of **literally no practical relevance** is totally lost.
 \item This also means that with a large enough sample size we can reject basically ANY null hypothesis (since the null hypothesis never exactly holds IRL, but it might be “close enough” that the violation of the null hypothesis is not important). 
 \item Want to write a paper saying Lucky Charms consumption is correlated w/blood type? W/a large enough sample size, you can get a small p-value.  (Provided there’s some super convoluted mechanism with some teeny effect size… which there probably is, b/c IRL null never holds)
\item THREE:  SEEK AND YOU SHALL FIND. If you look at your data for long enough, you will find something interesting, even if only by chance! 
In principle, we know that we need to perform a correction for multiple testing if we conduct a bunch of tests.
\item But in practice, what if we decide what test(s) to conduct AFTER we look at data?  Our p-value will be misleadingly small because we peeked at the data.  Pre-specifying our analysis plan in advance keeps us honest… but in reality, it’s hard to do!!!
\item Everyone is asking me about the mysterious and much-anticipated fourth rule of statistics. The answer is simple: we haven’t figured it out yet.... that’s the reason we need to do research in statistics
\end{itemize}

} 
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]\scriptsize
\frametitle{References}
\bibliography{bio}
\bibliographystyle{apalike}
%\bibliographystyle{flexbib}
\end{frame}  









%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
